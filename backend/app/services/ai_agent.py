"""
AI Agent service using LangChain 1.x and LangGraph 1.x

ä½¿ç”¨æœ€æ–°çš„ LangChain 1.1.0 å’Œ LangGraph 1.0.4 API
è¿™äº›ç‰ˆæœ¬æ˜¯ç”Ÿäº§å°±ç»ªçš„ç¨³å®šç‰ˆæœ¬ï¼Œæä¾›ï¼š
- ç¨³å®šçš„ API
- æ›´å¥½çš„æ€§èƒ½
- å®Œæ•´çš„ç±»å‹å®‰å…¨
- æŒä¹…åŒ–æ”¯æŒ
"""
from typing import TypedDict, Annotated, List, Dict, Any, Optional
from datetime import date, datetime

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.tools import tool
from langchain_chroma import Chroma
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session

from app.config import settings
from app.database import get_db
from app.repositories.plan_repository import PlanRepository
from app.repositories.preference_repository import PreferenceRepository
from app.services.vectorization_service import get_vectorization_service
from app.services.ai_prompts import (
    SYSTEM_PROMPT,
    PLAN_GENERATION_PROMPT,
    INTENT_UNDERSTANDING_PROMPT,
    format_user_preferences,
    format_historical_plans,
    format_similar_plans,
    format_knowledge
)


# Pydantic æ¨¡å‹ç”¨äºç»“æ„åŒ–è¾“å‡º
class MealItem(BaseModel):
    """é¤é£Ÿé¡¹ç›®"""
    name: str = Field(description="é¤é£Ÿåç§°")
    calories: float = Field(description="çƒ­é‡ï¼ˆå¡è·¯é‡Œï¼‰")
    items: List[str] = Field(description="é£Ÿç‰©åˆ—è¡¨")


class ExerciseItem(BaseModel):
    """è¿åŠ¨é¡¹ç›®"""
    name: str = Field(description="è¿åŠ¨åç§°")
    duration: int = Field(description="æ—¶é•¿ï¼ˆåˆ†é’Ÿï¼‰")
    calories: float = Field(description="æ¶ˆè€—çƒ­é‡ï¼ˆå¡è·¯é‡Œï¼‰")


class DietPlan(BaseModel):
    """é¥®é£Ÿè®­ç»ƒè®¡åˆ’"""
    date: str = Field(description="æ—¥æœŸ")
    meals: List[MealItem] = Field(description="é¤é£Ÿåˆ—è¡¨")
    exercises: List[ExerciseItem] = Field(description="è¿åŠ¨åˆ—è¡¨")
    reasoning: str = Field(description="è®¡åˆ’åˆ¶å®šçš„ç†ç”±å’Œè¯´æ˜")


# ============================================================================
# æ™ºèƒ½ä½“å·¥å…·å®šä¹‰
# ============================================================================

@tool
def get_user_preferences(user_id: Optional[int] = None) -> Dict[str, Any]:
    """
    è·å–ç”¨æˆ·åå¥½è®¾ç½®
    
    Args:
        user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º1ï¼‰
        
    Returns:
        ç”¨æˆ·åå¥½å­—å…¸ï¼ŒåŒ…å«ç›®æ ‡ã€è¿‡æ•é£Ÿç‰©ã€å¿Œå£ç­‰ä¿¡æ¯
    """
    try:
        db = next(get_db())
        repo = PreferenceRepository(db)
        
        # é»˜è®¤ä½¿ç”¨ç”¨æˆ·ID 1
        if user_id is None:
            user_id = 1
        
        preferences = repo.get_by_user_id(user_id)
        
        if preferences:
            return {
                "goal": preferences.goal,
                "allergies": preferences.allergies,
                "dislikes": preferences.dislikes,
                "target_calories": preferences.target_calories,
                "activity_level": preferences.activity_level
            }
        else:
            return {
                "goal": "ç»´æŒ",
                "allergies": [],
                "dislikes": [],
                "target_calories": 2000,
                "activity_level": "ä¸­ç­‰"
            }
    except Exception as e:
        return {"error": f"è·å–ç”¨æˆ·åå¥½å¤±è´¥: {str(e)}"}


@tool
def get_historical_plans(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    limit: int = 10
) -> List[Dict[str, Any]]:
    """
    è·å–å†å²è®¡åˆ’ï¼ˆç»“æ„åŒ–æŸ¥è¯¢ï¼‰
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
        limit: è¿”å›çš„æœ€å¤§è®°å½•æ•°
        
    Returns:
        å†å²è®¡åˆ’åˆ—è¡¨
    """
    try:
        db = next(get_db())
        repo = PlanRepository(db)
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥æœŸèŒƒå›´ï¼Œè·å–æœ€è¿‘çš„è®¡åˆ’
        if not start_date and not end_date:
            # è·å–æœ€è¿‘çš„è®¡åˆ’
            plans = repo.get_all()
            plans = sorted(plans, key=lambda x: x.date, reverse=True)[:limit]
        else:
            # æŒ‰æ—¥æœŸèŒƒå›´æŸ¥è¯¢
            if start_date:
                start = datetime.strptime(start_date, "%Y-%m-%d").date()
            else:
                start = date(2000, 1, 1)
            
            if end_date:
                end = datetime.strptime(end_date, "%Y-%m-%d").date()
            else:
                end = date.today()
            
            plans = []
            current = start
            while current <= end and len(plans) < limit:
                day_plans = repo.get_by_date(current)
                plans.extend(day_plans)
                current = date.fromordinal(current.toordinal() + 1)
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        result = []
        for plan in plans[:limit]:
            result.append({
                "id": plan.id,
                "date": plan.date.isoformat(),
                "type": plan.type,
                "name": plan.name,
                "calories": plan.calories,
                "duration": plan.duration,
                "completed": plan.completed
            })
        
        return result
    except Exception as e:
        return [{"error": f"è·å–å†å²è®¡åˆ’å¤±è´¥: {str(e)}"}]


@tool
def search_similar_plans(query: str, n_results: int = 5) -> List[Dict[str, Any]]:
    """
    æœç´¢ç›¸ä¼¼è®¡åˆ’ï¼ˆå‘é‡æ£€ç´¢ï¼‰
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        n_results: è¿”å›çš„ç»“æœæ•°é‡
        
    Returns:
        ç›¸ä¼¼è®¡åˆ’åˆ—è¡¨
    """
    try:
        vectorization_service = get_vectorization_service()
        results = vectorization_service.search_similar_plans(query, n_results)
        return results if results else []
    except Exception as e:
        return [{"error": f"æœç´¢ç›¸ä¼¼è®¡åˆ’å¤±è´¥: {str(e)}"}]


@tool
def search_conversations(query: str, n_results: int = 3) -> List[str]:
    """
    æœç´¢å†å²å¯¹è¯ï¼ˆå‘é‡æ£€ç´¢ï¼‰
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        n_results: è¿”å›çš„ç»“æœæ•°é‡
        
    Returns:
        ç›¸ä¼¼å¯¹è¯åˆ—è¡¨
    """
    try:
        vectorization_service = get_vectorization_service()
        # ä½¿ç”¨å‘é‡æ•°æ®åº“æœç´¢å¯¹è¯
        # æ³¨æ„ï¼šè¿™éœ€è¦åœ¨ vectorization_service ä¸­å®ç°
        # æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨
        return []
    except Exception as e:
        return [f"æœç´¢å¯¹è¯å¤±è´¥: {str(e)}"]


@tool
def search_knowledge(topic: str, knowledge_type: str = "both", n_results: int = 3) -> str:
    """
    æœç´¢è¥å…»å’Œè¿åŠ¨çŸ¥è¯†åº“
    
    Args:
        topic: æœç´¢ä¸»é¢˜
        knowledge_type: çŸ¥è¯†ç±»å‹ ("nutrition", "exercise", "both")
        n_results: è¿”å›çš„ç»“æœæ•°é‡
        
    Returns:
        ç›¸å…³çŸ¥è¯†æ–‡æœ¬
    """
    try:
        vectorization_service = get_vectorization_service()
        
        results = []
        
        if knowledge_type in ["nutrition", "both"]:
            nutrition_knowledge = vectorization_service.get_nutrition_knowledge(
                topic, n_results
            )
            if nutrition_knowledge:
                results.append(f"è¥å…»çŸ¥è¯†ï¼š\n{nutrition_knowledge}")
        
        if knowledge_type in ["exercise", "both"]:
            exercise_knowledge = vectorization_service.get_exercise_knowledge(
                topic, n_results
            )
            if exercise_knowledge:
                results.append(f"è¿åŠ¨çŸ¥è¯†ï¼š\n{exercise_knowledge}")
        
        return "\n\n".join(results) if results else "æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†"
    except Exception as e:
        return f"æœç´¢çŸ¥è¯†å¤±è´¥: {str(e)}"


@tool
def calculate_nutrition(
    weight: float,
    height: float,
    age: int,
    gender: str,
    activity_level: str,
    goal: str
) -> Dict[str, Any]:
    """
    è®¡ç®—è¥å…»éœ€æ±‚
    
    Args:
        weight: ä½“é‡ï¼ˆkgï¼‰
        height: èº«é«˜ï¼ˆcmï¼‰
        age: å¹´é¾„
        gender: æ€§åˆ« ("male" æˆ– "female")
        activity_level: æ´»åŠ¨æ°´å¹³ ("ä½", "ä¸­ç­‰", "é«˜")
        goal: ç›®æ ‡ ("å‡è„‚", "å¢è‚Œ", "ç»´æŒ")
        
    Returns:
        è¥å…»éœ€æ±‚å­—å…¸ï¼ŒåŒ…å«æ¯æ—¥çƒ­é‡ã€è›‹ç™½è´¨ã€ç¢³æ°´ã€è„‚è‚ªç­‰
    """
    try:
        # è®¡ç®—åŸºç¡€ä»£è°¢ç‡ (BMR) - ä½¿ç”¨ Mifflin-St Jeor å…¬å¼
        if gender.lower() in ["male", "ç”·"]:
            bmr = 10 * weight + 6.25 * height - 5 * age + 5
        else:
            bmr = 10 * weight + 6.25 * height - 5 * age - 161
        
        # æ ¹æ®æ´»åŠ¨æ°´å¹³è°ƒæ•´
        activity_multipliers = {
            "ä½": 1.2,
            "ä¸­ç­‰": 1.55,
            "é«˜": 1.9
        }
        multiplier = activity_multipliers.get(activity_level, 1.55)
        tdee = bmr * multiplier
        
        # æ ¹æ®ç›®æ ‡è°ƒæ•´çƒ­é‡
        if goal == "å‡è„‚":
            target_calories = tdee - 500  # æ¯æ—¥å‡å°‘500å¡è·¯é‡Œ
            protein_ratio = 0.30  # 30% è›‹ç™½è´¨
            carb_ratio = 0.40     # 40% ç¢³æ°´
            fat_ratio = 0.30      # 30% è„‚è‚ª
        elif goal == "å¢è‚Œ":
            target_calories = tdee + 300  # æ¯æ—¥å¢åŠ 300å¡è·¯é‡Œ
            protein_ratio = 0.30
            carb_ratio = 0.50
            fat_ratio = 0.20
        else:  # ç»´æŒ
            target_calories = tdee
            protein_ratio = 0.25
            carb_ratio = 0.50
            fat_ratio = 0.25
        
        # è®¡ç®—å®é‡è¥å…»ç´ ï¼ˆå…‹ï¼‰
        protein_grams = (target_calories * protein_ratio) / 4  # 1gè›‹ç™½è´¨ = 4å¡è·¯é‡Œ
        carb_grams = (target_calories * carb_ratio) / 4        # 1gç¢³æ°´ = 4å¡è·¯é‡Œ
        fat_grams = (target_calories * fat_ratio) / 9          # 1gè„‚è‚ª = 9å¡è·¯é‡Œ
        
        return {
            "bmr": round(bmr, 1),
            "tdee": round(tdee, 1),
            "target_calories": round(target_calories, 1),
            "protein_grams": round(protein_grams, 1),
            "carb_grams": round(carb_grams, 1),
            "fat_grams": round(fat_grams, 1),
            "protein_ratio": protein_ratio,
            "carb_ratio": carb_ratio,
            "fat_ratio": fat_ratio
        }
    except Exception as e:
        return {"error": f"è®¡ç®—è¥å…»éœ€æ±‚å¤±è´¥: {str(e)}"}


# ============================================================================
# LangGraph çŠ¶æ€å®šä¹‰
# ============================================================================

class AgentState(TypedDict):
    """æ™ºèƒ½ä½“çŠ¶æ€"""
    messages: Annotated[list, add_messages]  # è‡ªåŠ¨åˆå¹¶æ¶ˆæ¯
    user_preferences: Dict[str, Any]
    historical_plans: List[Dict[str, Any]]
    retrieved_context: Dict[str, Any]
    generated_plan: Dict[str, Any]
    current_step: str


class DietTrainingAgent:
    """é¥®é£Ÿè®­ç»ƒè®¡åˆ’ AI æ™ºèƒ½ä½“"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ™ºèƒ½ä½“ç»„ä»¶"""
        # åˆå§‹åŒ– LLMï¼ˆæ”¯æŒè‡ªå®šä¹‰API baseï¼‰
        llm_kwargs = {
            "model": settings.openai_model_name,
            "temperature": 0.7,
            "api_key": settings.openai_api_key
        }
        if settings.openai_api_base:
            llm_kwargs["base_url"] = settings.openai_api_base
        
        self.llm = ChatOpenAI(**llm_kwargs)
        
        # åˆå§‹åŒ– Embeddingsï¼ˆæ”¯æŒè‡ªå®šä¹‰API baseå’Œæ¨¡å‹ï¼‰
        embedding_kwargs = {
            "model": settings.openai_embedding_model,  # ä½¿ç”¨é…ç½®çš„embeddingæ¨¡å‹
            "api_key": settings.openai_api_key
        }
        if settings.openai_api_base:
            embedding_kwargs["base_url"] = settings.openai_api_base
        
        self.embeddings = OpenAIEmbeddings(**embedding_kwargs)
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        self.vectorstore = Chroma(
            collection_name="diet_training",
            embedding_function=self.embeddings,
            persist_directory=settings.chroma_persist_directory
        )
        
        # æ³¨å†Œå·¥å…·
        self.tools = [
            get_user_preferences,
            get_historical_plans,
            search_similar_plans,
            search_conversations,
            search_knowledge,
            calculate_nutrition
        ]
        
        # å°†å·¥å…·ç»‘å®šåˆ° LLM
        self.llm_with_tools = self.llm.bind_tools(self.tools)
        
        # åˆ›å»ºæ™ºèƒ½ä½“å›¾
        self.agent = self._create_agent()
    
    def _call_tools(self, state: AgentState) -> AgentState:
        """è°ƒç”¨å·¥å…·èŠ‚ç‚¹"""
        messages = state["messages"]
        last_message = messages[-1]
        
        # ä½¿ç”¨å¸¦å·¥å…·çš„ LLM å¤„ç†æ¶ˆæ¯
        response = self.llm_with_tools.invoke(messages)
        
        state["messages"].append(response)
        state["current_step"] = "tools_called"
        
        return state
    
    def _understand_intent(self, state: AgentState) -> AgentState:
        """ç†è§£ç”¨æˆ·æ„å›¾"""
        messages = state["messages"]
        last_message = messages[-1].content if messages else ""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", INTENT_UNDERSTANDING_PROMPT),
            ("human", "{input}")
        ])
        
        chain = prompt | self.llm
        response = chain.invoke({"input": last_message})
        
        state["current_step"] = "intent_understood"
        state["messages"].append(AIMessage(content=response.content))
        
        return state
    
    def _retrieve_vector_context(self, state: AgentState) -> AgentState:
        """ä»å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ï¼ˆå‘é‡æ£€ç´¢ï¼‰"""
        messages = state["messages"]
        last_user_message = next(
            (m.content for m in reversed(messages) if isinstance(m, HumanMessage)),
            ""
        )
        
        # è¯­ä¹‰æœç´¢ç›¸ä¼¼å¯¹è¯
        similar_conversations = self.vectorstore.similarity_search(
            last_user_message,
            k=3,
            filter={"type": "conversation"}
        )
        
        # è¯­ä¹‰æœç´¢ç›¸ä¼¼è®¡åˆ’
        similar_plans = self.vectorstore.similarity_search(
            last_user_message,
            k=3,
            filter={"type": "plan"}
        )
        
        # æœç´¢çŸ¥è¯†åº“
        knowledge = self.vectorstore.similarity_search(
            last_user_message,
            k=2,
            filter={"type": "knowledge"}
        )
        
        # æ›´æ–°æ£€ç´¢ä¸Šä¸‹æ–‡
        if "retrieved_context" not in state:
            state["retrieved_context"] = {}
        
        state["retrieved_context"]["similar_conversations"] = [
            doc.page_content for doc in similar_conversations
        ]
        state["retrieved_context"]["similar_plans"] = [
            doc.page_content for doc in similar_plans
        ]
        state["retrieved_context"]["knowledge"] = [
            doc.page_content for doc in knowledge
        ]
        
        state["current_step"] = "vector_context_retrieved"
        
        return state
    
    def _retrieve_structured_data(self, state: AgentState) -> AgentState:
        """æ£€ç´¢ç»“æ„åŒ–æ•°æ®ï¼ˆæ•°æ®åº“æŸ¥è¯¢ï¼‰"""
        try:
            # è·å–ç”¨æˆ·åå¥½
            user_prefs = get_user_preferences.invoke({})
            state["user_preferences"] = user_prefs
            
            # è·å–æœ€è¿‘çš„å†å²è®¡åˆ’
            historical = get_historical_plans.invoke({"limit": 5})
            state["historical_plans"] = historical
            
            # æ›´æ–°æ£€ç´¢ä¸Šä¸‹æ–‡
            if "retrieved_context" not in state:
                state["retrieved_context"] = {}
            
            state["retrieved_context"]["user_preferences"] = user_prefs
            state["retrieved_context"]["historical_plans"] = historical
            
            state["current_step"] = "structured_data_retrieved"
            
        except Exception as e:
            state["current_step"] = "error"
            state["messages"].append(
                AIMessage(content=f"æ£€ç´¢ç»“æ„åŒ–æ•°æ®æ—¶å‡ºé”™ï¼š{str(e)}")
            )
        
        return state
    
    def _generate_plan(self, state: AgentState) -> AgentState:
        """ç”Ÿæˆé¥®é£Ÿè®­ç»ƒè®¡åˆ’æˆ–æ™®é€šå¯¹è¯å“åº”"""
        messages = state["messages"]
        context = state["retrieved_context"]
        preferences = state["user_preferences"]
        historical = state["historical_plans"]
        
        last_user_message = next(
            (m.content for m in reversed(messages) if isinstance(m, HumanMessage)),
            ""
        )
        
        # åˆ¤æ–­ç”¨æˆ·æ˜¯å¦éœ€è¦ç”Ÿæˆè®¡åˆ’
        plan_keywords = ["è®¡åˆ’", "é¥®é£Ÿ", "é¤é£Ÿ", "è¿åŠ¨", "é”»ç‚¼", "å¥èº«", "å‡è„‚", "å¢è‚Œ", "ç”Ÿæˆ", "å¸®æˆ‘"]
        needs_plan = any(keyword in last_user_message for keyword in plan_keywords)
        
        if needs_plan:
            # ç”Ÿæˆç»“æ„åŒ–è®¡åˆ’
            try:
                parser = JsonOutputParser(pydantic_object=DietPlan)
                
                # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯
                formatted_preferences = format_user_preferences(preferences)
                formatted_historical = format_historical_plans(historical)
                formatted_similar = format_similar_plans(context.get("similar_plans", []))
                formatted_knowledge = format_knowledge(context.get("knowledge", []))
                
                # æ„å»ºæç¤ºè¯
                prompt = ChatPromptTemplate.from_messages([
                    ("system", PLAN_GENERATION_PROMPT),
                    ("human", "{input}")
                ])
                
                chain = prompt | self.llm | parser
                
                result = chain.invoke({
                    "input": last_user_message,
                    "user_preferences": formatted_preferences,
                    "historical_plans": formatted_historical,
                    "similar_plans": formatted_similar,
                    "knowledge": formatted_knowledge,
                    "format_instructions": parser.get_format_instructions()
                })
                
                state["generated_plan"] = result
                state["current_step"] = "plan_generated"
                
                # æ·»åŠ  AI å“åº”æ¶ˆæ¯
                response_text = f"æˆ‘å·²ç»ä¸ºæ‚¨ç”Ÿæˆäº†é¥®é£Ÿè®­ç»ƒè®¡åˆ’ã€‚\n\n{result.get('reasoning', '')}"
                state["messages"].append(AIMessage(content=response_text))
                
            except Exception as e:
                state["current_step"] = "error"
                state["messages"].append(
                    AIMessage(content=f"ç”Ÿæˆè®¡åˆ’æ—¶å‡ºé”™ï¼š{str(e)}")
                )
        else:
            # æ™®é€šå¯¹è¯å“åº”
            try:
                prompt = ChatPromptTemplate.from_messages([
                    ("system", SYSTEM_PROMPT),
                    ("human", "{input}")
                ])
                
                chain = prompt | self.llm
                
                response = chain.invoke({"input": last_user_message})
                
                state["current_step"] = "response_generated"
                state["messages"].append(AIMessage(content=response.content))
                
            except Exception as e:
                state["current_step"] = "error"
                state["messages"].append(
                    AIMessage(content=f"ç”Ÿæˆå“åº”æ—¶å‡ºé”™ï¼š{str(e)}")
                )
        
        return state
    
    def _validate_plan(self, state: AgentState) -> AgentState:
        """éªŒè¯è®¡åˆ’çš„åˆç†æ€§"""
        plan = state["generated_plan"]
        
        if not plan:
            state["current_step"] = "validation_failed"
            return state
        
        # éªŒè¯çƒ­é‡å¹³è¡¡
        total_intake = sum(meal.get("calories", 0) for meal in plan.get("meals", []))
        total_burned = sum(ex.get("calories", 0) for ex in plan.get("exercises", []))
        
        # åŸºæœ¬éªŒè¯è§„åˆ™
        if total_intake < 1000 or total_intake > 5000:
            state["messages"].append(
                AIMessage(content="è­¦å‘Šï¼šæ€»çƒ­é‡æ‘„å…¥å¯èƒ½ä¸åˆç†ï¼Œè¯·è°ƒæ•´ã€‚")
            )
        
        if total_burned > total_intake * 0.5:
            state["messages"].append(
                AIMessage(content="æç¤ºï¼šè¿åŠ¨æ¶ˆè€—è¾ƒå¤§ï¼Œæ³¨æ„è¡¥å……èƒ½é‡ã€‚")
            )
        
        state["current_step"] = "plan_validated"
        
        return state
    
    def _format_response(self, state: AgentState) -> AgentState:
        """æ ¼å¼åŒ–å“åº”"""
        plan = state["generated_plan"]
        
        if not plan:
            state["current_step"] = "response_formatted"
            return state
        
        # æ ¼å¼åŒ–è®¡åˆ’ä¸ºæ˜“è¯»çš„æ–‡æœ¬
        formatted_text = f"ğŸ“… æ—¥æœŸ: {plan.get('date', 'ä»Šå¤©')}\n\n"
        
        # æ ¼å¼åŒ–é¤é£Ÿ
        if plan.get("meals"):
            formatted_text += "ğŸ½ï¸ é¤é£Ÿå®‰æ’:\n"
            for i, meal in enumerate(plan.get("meals", []), 1):
                formatted_text += f"{i}. {meal.get('name', 'æœªå‘½åé¤é£Ÿ')}\n"
                formatted_text += f"   çƒ­é‡: {meal.get('calories', 0)}å¡è·¯é‡Œ\n"
                if meal.get("items"):
                    formatted_text += f"   é£Ÿç‰©: {', '.join(meal.get('items', []))}\n"
                formatted_text += "\n"
        
        # æ ¼å¼åŒ–è¿åŠ¨
        if plan.get("exercises"):
            formatted_text += "ğŸ’ª è¿åŠ¨å®‰æ’:\n"
            for i, exercise in enumerate(plan.get("exercises", []), 1):
                formatted_text += f"{i}. {exercise.get('name', 'æœªå‘½åè¿åŠ¨')}\n"
                formatted_text += f"   æ—¶é•¿: {exercise.get('duration', 0)}åˆ†é’Ÿ\n"
                formatted_text += f"   æ¶ˆè€—: {exercise.get('calories', 0)}å¡è·¯é‡Œ\n\n"
        
        # æ·»åŠ ç†ç”±
        if plan.get("reasoning"):
            formatted_text += f"ğŸ’¡ è®¡åˆ’è¯´æ˜:\n{plan.get('reasoning')}\n"
        
        # æ›´æ–°æœ€åä¸€æ¡ AI æ¶ˆæ¯æˆ–æ·»åŠ æ–°æ¶ˆæ¯
        if state["messages"] and isinstance(state["messages"][-1], AIMessage):
            state["messages"][-1] = AIMessage(content=formatted_text)
        else:
            state["messages"].append(AIMessage(content=formatted_text))
        
        state["current_step"] = "response_formatted"
        
        return state
    
    def _store_to_vector_db(self, state: AgentState) -> AgentState:
        """å°†å¯¹è¯å’Œè®¡åˆ’å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“"""
        from langchain_core.documents import Document
        
        # å­˜å‚¨å¯¹è¯
        messages = state["messages"]
        for msg in messages[-2:]:  # åªå­˜å‚¨æœ€è¿‘çš„å¯¹è¯
            if isinstance(msg, (HumanMessage, AIMessage)):
                doc = Document(
                    page_content=msg.content,
                    metadata={
                        "role": "user" if isinstance(msg, HumanMessage) else "assistant",
                        "type": "conversation",
                        "timestamp": date.today().isoformat()
                    }
                )
                self.vectorstore.add_documents([doc])
        
        # å­˜å‚¨ç”Ÿæˆçš„è®¡åˆ’
        plan = state["generated_plan"]
        if plan:
            content = f"æ—¥æœŸ: {plan.get('date', '')}\n"
            content += f"é¤é£Ÿ: {', '.join([m.get('name', '') for m in plan.get('meals', [])])}\n"
            content += f"è¿åŠ¨: {', '.join([e.get('name', '') for e in plan.get('exercises', [])])}\n"
            content += f"ç†ç”±: {plan.get('reasoning', '')}"
            
            doc = Document(
                page_content=content,
                metadata={
                    "type": "plan",
                    "date": plan.get("date", ""),
                    "timestamp": date.today().isoformat()
                }
            )
            self.vectorstore.add_documents([doc])
        
        state["current_step"] = "stored"
        
        return state
    
    def _execute_tools(self, state: AgentState) -> AgentState:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        messages = state["messages"]
        last_message = messages[-1]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if not hasattr(last_message, "tool_calls") or not last_message.tool_calls:
            return state
        
        # åˆ›å»ºå·¥å…·æ˜ å°„
        tool_map = {tool.name: tool for tool in self.tools}
        
        # æ‰§è¡Œæ¯ä¸ªå·¥å…·è°ƒç”¨
        for tool_call in last_message.tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]
            
            if tool_name in tool_map:
                try:
                    # è°ƒç”¨å·¥å…·
                    tool_result = tool_map[tool_name].invoke(tool_args)
                    
                    # æ·»åŠ å·¥å…·ç»“æœæ¶ˆæ¯
                    tool_message = ToolMessage(
                        content=str(tool_result),
                        tool_call_id=tool_call["id"]
                    )
                    state["messages"].append(tool_message)
                except Exception as e:
                    # æ·»åŠ é”™è¯¯æ¶ˆæ¯
                    error_message = ToolMessage(
                        content=f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}",
                        tool_call_id=tool_call["id"]
                    )
                    state["messages"].append(error_message)
        
        state["current_step"] = "tools_executed"
        return state
    
    def _should_continue(self, state: AgentState) -> str:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­è°ƒç”¨å·¥å…·"""
        messages = state["messages"]
        last_message = messages[-1]
        
        # å¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯åŒ…å«å·¥å…·è°ƒç”¨ï¼Œç»§ç»­æ‰§è¡Œå·¥å…·
        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            return "tools"
        
        # å¦åˆ™ç»§ç»­åˆ°ä¸‹ä¸€æ­¥
        return "continue"
    
    def _create_agent(self) -> Any:
        """åˆ›å»º LangGraph æ™ºèƒ½ä½“ï¼ˆRAG æµç¨‹ï¼‰"""
        workflow = StateGraph(AgentState)
        
        # æ·»åŠ èŠ‚ç‚¹ - RAG æµç¨‹
        workflow.add_node("call_tools", self._call_tools)
        workflow.add_node("tools", self._execute_tools)
        workflow.add_node("understand_intent", self._understand_intent)
        workflow.add_node("retrieve_vector_context", self._retrieve_vector_context)
        workflow.add_node("retrieve_structured_data", self._retrieve_structured_data)
        workflow.add_node("generate_plan", self._generate_plan)
        workflow.add_node("validate_plan", self._validate_plan)
        workflow.add_node("format_response", self._format_response)
        workflow.add_node("store_to_vector_db", self._store_to_vector_db)
        
        # æ·»åŠ è¾¹ - æ„å»º RAG å·¥ä½œæµ
        workflow.add_edge(START, "call_tools")
        
        # æ¡ä»¶è¾¹ï¼šæ ¹æ®æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·å†³å®šä¸‹ä¸€æ­¥
        workflow.add_conditional_edges(
            "call_tools",
            self._should_continue,
            {
                "tools": "tools",
                "continue": "understand_intent"
            }
        )
        
        # å·¥å…·æ‰§è¡Œåè¿”å›åˆ° call_tools
        workflow.add_edge("tools", "call_tools")
        
        # RAG æ£€ç´¢æµç¨‹
        workflow.add_edge("understand_intent", "retrieve_vector_context")
        workflow.add_edge("retrieve_vector_context", "retrieve_structured_data")
        
        # ç”Ÿæˆå’ŒéªŒè¯æµç¨‹
        workflow.add_edge("retrieve_structured_data", "generate_plan")
        workflow.add_edge("generate_plan", "validate_plan")
        workflow.add_edge("validate_plan", "format_response")
        
        # å­˜å‚¨å’Œç»“æŸ
        workflow.add_edge("format_response", "store_to_vector_db")
        workflow.add_edge("store_to_vector_db", END)
        
        # ç¼–è¯‘å›¾
        return workflow.compile()
    
    def generate_plan(
        self,
        user_input: str,
        user_preferences: Dict[str, Any],
        historical_plans: List[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆé¥®é£Ÿè®­ç»ƒè®¡åˆ’
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            user_preferences: ç”¨æˆ·åå¥½
            historical_plans: å†å²è®¡åˆ’
            
        Returns:
            ç”Ÿæˆçš„è®¡åˆ’
        """
        initial_state = {
            "messages": [HumanMessage(content=user_input)],
            "user_preferences": user_preferences or {},
            "historical_plans": historical_plans or [],
            "retrieved_context": {},
            "generated_plan": {},
            "current_step": "start"
        }
        
        # è°ƒç”¨æ™ºèƒ½ä½“
        result = self.agent.invoke(initial_state)
        
        return {
            "plan": result["generated_plan"],
            "messages": [
                {"role": "user" if isinstance(m, HumanMessage) else "assistant", "content": m.content}
                for m in result["messages"]
            ]
        }
    
    async def generate_plan_stream(
        self,
        user_input: str,
        user_preferences: Dict[str, Any]
    ):
        """
        æµå¼ç”Ÿæˆè®¡åˆ’
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            user_preferences: ç”¨æˆ·åå¥½
            
        Yields:
            ç”Ÿæˆçš„å†…å®¹å—
        """
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªé¥®é£Ÿè®­ç»ƒè®¡åˆ’åŠ©æ‰‹ã€‚ç”¨æˆ·åå¥½ï¼š{preferences}"),
            ("human", "{input}")
        ])
        
        chain = prompt | self.llm
        
        async for chunk in chain.astream({
            "input": user_input,
            "preferences": user_preferences
        }):
            if chunk.content:
                yield chunk.content


# ============================================================================
# å…¨å±€æ™ºèƒ½ä½“å®ä¾‹
# ============================================================================

_agent_instance: Optional[DietTrainingAgent] = None


def get_ai_agent() -> DietTrainingAgent:
    """
    è·å–AIæ™ºèƒ½ä½“å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    Returns:
        DietTrainingAgent å®ä¾‹
    """
    global _agent_instance
    
    if _agent_instance is None:
        _agent_instance = DietTrainingAgent()
    
    return _agent_instance
